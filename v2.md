1) New span taxonomy (V2)

Use these manual span names (examples):

agent.route

agent.model.select

rag.retrieve

rag.rerank

rag.assemble_context

tool.<tool_name>.call (e.g., tool.cosmos.query, tool.storage.read)

eval.run

eval.score

safety.guardrail.check

Keep OpenLLMetry auto spans for underlying LLM/tool SDK calls as children.

2) New common attributes (V2)

Add these attributes where applicable:

workflow.name (or graph.name)

workflow.step (optional, numeric/string)

tool.name

tool.kind (database|storage|http|search|custom)

rag.index.name

rag.top_k

rag.retrieved_count

rag.reranked_count

eval.name

eval.metric

eval.threshold

eval.result (pass|fail)

gen_ai.provider (e.g., azure_openai)

gen_ai.request.model (already in V1, keep)

3) V2 metrics catalog (new metrics)
A) RAG metrics
Metric name	Type	Unit	Purpose	Required dimensions
rag_retrieval_requests_total	Counter	1	Number of retrieval operations	agent.name, http.route, rag.index.name, status
rag_retrieval_latency_ms	Histogram	ms	Retrieval latency distribution	agent.name, http.route, rag.index.name, status
rag_retrieved_documents_total	Counter	1	Total docs returned by retriever	agent.name, http.route, rag.index.name
rag_rerank_latency_ms	Histogram	ms	Reranker latency	agent.name, http.route, status
rag_context_tokens_total	Counter	1	Tokens inserted as retrieved context	agent.name, http.route, gen_ai.request.model
rag_no_result_total	Counter	1	Retrieval calls with zero results	agent.name, http.route, rag.index.name
B) Tool metrics
Metric name	Type	Unit	Purpose	Required dimensions
ai_tool_calls_total	Counter	1	Total tool invocations	agent.name, http.route, tool.name, tool.kind, status
ai_tool_latency_ms	Histogram	ms	Tool latency distribution	agent.name, http.route, tool.name, tool.kind, status
ai_tool_errors_total	Counter	1	Tool failures	agent.name, http.route, tool.name, tool.kind, error.type
ai_tool_timeout_total	Counter	1	Tool timeout incidents	agent.name, http.route, tool.name
C) Multi-model orchestration metrics
Metric name	Type	Unit	Purpose	Required dimensions
ai_model_invocations_total	Counter	1	Number of LLM invocations per model	agent.name, http.route, gen_ai.provider, gen_ai.request.model, status
ai_model_latency_ms	Histogram	ms	Per-model latency	agent.name, http.route, gen_ai.request.model, status
ai_model_fallback_total	Counter	1	Model fallback occurrences	agent.name, http.route, from.model, to.model, reason
ai_model_selection_total	Counter	1	Routing decisions to selected model	agent.name, http.route, selected.model, selection.policy
D) Evaluation metrics
Metric name	Type	Unit	Purpose	Required dimensions
ai_eval_runs_total	Counter	1	Total eval executions	agent.name, eval.name, status
ai_eval_pass_total	Counter	1	Eval pass count	agent.name, eval.name, eval.metric
ai_eval_fail_total	Counter	1	Eval fail count	agent.name, eval.name, eval.metric
ai_eval_score	Histogram	1	Distribution of eval scores	agent.name, eval.name, eval.metric
ai_eval_latency_ms	Histogram	ms	Eval runtime latency	agent.name, eval.name, status
E) Streaming metrics (if applicable)
Metric name	Type	Unit	Purpose	Required dimensions
ai_stream_responses_total	Counter	1	Count of streaming responses	agent.name, http.route, gen_ai.request.model, status
ai_stream_first_token_latency_ms	Histogram	ms	Time-to-first-token	agent.name, http.route, gen_ai.request.model, status
ai_stream_tokens_total	Counter	1	Streamed token count	agent.name, http.route, gen_ai.request.model
4) Enumerations for V2 (standardize now)
status

ok

error

tool.kind

database

storage

http

search

custom

eval.result

pass

fail

reason (for fallback)

timeout

rate_limit

quality_guardrail

cost_policy

availability

5) Recommended alert set (V2)

Latency regression
ai_request_latency_ms p95 above threshold by http.route

Tool instability
ai_tool_errors_total / ai_tool_calls_total above X%

RAG quality risk proxy
rag_no_result_total spike or rag_retrieved_documents_total drop

Model fallback surge
ai_model_fallback_total sudden increase

Eval degradation
ai_eval_fail_total or drop in ai_eval_score median

6) Minimal code changes to adopt V2

Keep V1 module as-is.

Add new instruments in init_observability() for V2 metrics.

Add helper functions, e.g.:

record_rag(...)

record_tool_call(...)

record_model_selection(...)

record_eval(...)

Emit from each stage span/function.

Keep attribute keys consistent with the tables above.
